#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

# this file could be generated by mapreduce or spark.
import avro.schema
import random

from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter

schema = avro.schema.parse(open("event2.avsc").read())

def gen_single_day_data(date, schema):
    writer = DataFileWriter(open("events2-{}.avro".format(date), "w"), DatumWriter(), schema)
    N = 10 ** 5
    for i in xrange(0, N):
        tags = ["t{}".format(random.randint(1, 10)) for x in range(0, 4)]
        (tag1, tag2, tag3, tag4) = tags
        cookie = 'CK.{}'.format(random.randint(1, 10 ** 5))
        writer.append({"tag1":tag1, "tag2":tag2, "tag3": tag3, "tag4":tag4, "date":date, "cookie":cookie, "count": 1})
    writer.close()

def gen_data():
    for x in range(0, 10):
        date = '201509%02d' % (x)
        gen_single_day_data(date, schema)

def gen_load_cmd():
    for x in range(0, 10):
        date = '201509%02d' % (x)
        cmd = "LOAD DATA INPATH 'hdfs://localhost:8020/user/dirlt/hive/data/event2/events2-{}.avro' OVERWRITE INTO TABLE event2 PARTITION(day={});".format(date, date)
        cmd2 = 'echo "{}" | hive'.format(cmd)
        print cmd2

#gen_data()
gen_load_cmd()
